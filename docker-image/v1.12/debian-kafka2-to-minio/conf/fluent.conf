
# AUTOMATICALLY GENERATED
# DO NOT EDIT THIS FILE DIRECTLY, USE /templates/conf/fluent.conf.erb

# @include "#{ENV['FLUENTD_SYSTEMD_CONF'] || 'systemd'}.conf"
# @include "#{ENV['FLUENTD_PROMETHEUS_CONF'] || 'prometheus'}.conf"
# @include kubernetes.conf
# @include conf.d/*.conf

<source>
  @type kafka_group

  brokers "#{ENV['FLUENT_KAFKA2_BROKERS']}"
  consumer_group "#{ENV['FLUENT_KAFKA2_CONSUME_GROUP']}"
  topics "#{ENV['FLUENT_KAFKA2_TOPICS']}"
  format json
  message_key message
  # kafka_mesasge_key <key (Optional, If specified, set kafka's message key to this key)>
  add_headers false
  # add_prefix <tag prefix (Optional)>
  # add_suffix <tag suffix (Optional)>
  # retry_emit_limit <Wait retry_emit_limit x 1s when BuffereQueueLimitError happens. The default is nil and it means waiting until BufferQueueLimitError is resolved>
  # use_record_time (Deprecated. Use 'time_source record' instead.) <If true, replace event time with contents of 'time' field of fetched record>
  time_source  kafka    #<source for message timestamp (now|kafka|record)> :default => now
  # time_format <string (Optional when use_record_time is used)>

  # ruby-kafka consumer options
  # max_bytes               (integer) :default => 1048576
  # max_wait_time           (integer) :default => nil (Use default of ruby-kafka)
  # min_bytes               (integer) :default => nil (Use default of ruby-kafka)
  # offset_commit_interval  (integer) :default => nil (Use default of ruby-kafka)
  # offset_commit_threshold (integer) :default => nil (Use default of ruby-kafka)
  # fetcher_max_queue_size  (integer) :default => nil (Use default of ruby-kafka)
  # start_from_beginning    (bool)    :default => true
</source>

# <match **>
#  @type stdout
# </match>

<match **>
  @type s3
  aws_key_id "#{ENV['FLUENT_S3_ACCESS_KEY']}"                         # The access key for Minio
  aws_sec_key "#{ENV['FLUENT_S3_SECRET_KEY']}"                        # The secret key for Minio
  s3_bucket "#{ENV['FLUENT_S3_BUCKET']}"                              # The bucket to store the log data
  s3_endpoint "#{ENV['FLUENT_S3_ENDPOINT']}"                          # The endpoint URL (like "http://localhost:9000/")
  # s3_region us-east-1                                               # See the region settings of your Minio server
  path "#{ENV['FLUENT_S3_LOG_PATH']}"                                 # This prefix is added to each file
  force_path_style true                                               # This prevents AWS SDK from breaking endpoint URL
  time_slice_format "#{ENV['FLUENT_S3_TIME_SLICE_FORMAT']}"           # This timestamp is added to each file name
  ssl_verify_peer "#{ENV['FLUENT_S3_SSL_VERIFY_PEER']}"

  <buffer time>
    @type file
    path /var/log/td-agent/s3
    timekey "#{ENV['FLUENT_S3_BUFFER_TIME_KEY']}"                     # Flush the accumulated chunks every hour
    timekey_wait "#{ENV['FLUENT_S3_BUFFER_WAIT']}"                    # Wait for 60 seconds before flushing
    timekey_use_utc true                                              # Use this option if you prefer UTC timestamps
    chunk_limit_size "#{ENV['FLUENT_S3_BUFFER_CHUNK_LIMIT_SIZE']}"    # The maximum size of each chunk
  </buffer>
</match>